---
title: "Харинаев Артём 316 группа 11.10.21"
output:
  html_document:
    df_print: paged
---
## 1. Оценка максимального правдоподобия

Построим оценку для дисперсии нормального распределения
```{r}
set.seed(151021)
sample <- rnorm(1, mean=10, sd=2)
minus_likelihood <- function(theta){
  dnorm(sample, mean=10, sd=theta)*-1
}
```
```{r}
nlm(minus_likelihood, p=1, stepmax=0.5)$estimate
```

Оценка довольно близка к истинному значению дисперсии (2)

## 2. Анализ выборок из нормального распределения
### 2.1 Выборки малого размера
#### 2.1.1 Генерирование и визуализация 
```{r}
set.seed(151021)

s.1 <- rnorm(50, 0, 25)
s.2 <- rnorm(75, 50, 10)
s.3 <- rnorm(100, 30, 10)

hist(s.3, col='lightblue', xlim=c(-80,80), ylim=c(0,0.05), freq=FALSE, main='3 нормально распределенных выборки', xlab='', ylab='Плотность')
hist(s.2, col='lightgreen', add=TRUE, freq=FALSE)
hist(s.1, col='salmon', add=TRUE, freq=FALSE)

lines(density(s.3), col='blue3', lwd=2)
lines(density(s.2), col='green3', lwd=2)
lines(density(s.1), col='red3', lwd=2)

legend('topleft', 
       legend=c('1. ~ N(0,25), n=50', '2. ~ N(50,10), n=75', '3. ~ N(30,10), n=100'), 
       fill=c('red','green', 'blue'))
```

#### 2.1.2 Теоретические и эмпирические функции рапределения и плотности
```{r}
theor_empiric <- function(cur, mean, sd){
  par(mfrow=c(1,2))
  plot(sort(cur), pnorm(sort(cur),mean,sd), type='l', col='red', main='Функция распределения',
        xlab='', ylab='')
  plot(ecdf(cur), add=TRUE)
  plot(density(cur), main='Плотность распределения', xlab='', ylab='')
  lines(sort(cur), dnorm(sort(cur),mean,sd), type='l', col='red')
}
```
```{r}
rtheor_empiric(s.1, 0, 25)
theor_empiric(s.2, 50, 10)
theor_empiric(s.3, 30, 10)
```

Даже на небольших данных заметно, что с увеличением объема выборки, эмпирические данные приближаются к теоретическим

#### 2.1.3 Квантили
```{r}
quantile <- function(x){
  qqnorm(x)
  qqline(x, col='red')
}
```
```{r}
quantile(s.1)
quantile(s.2)
quantile(s.3)
```

Здесь так же заметна разница от размера выборки (чем больше объем, тем ближе точки к прямой)

#### 2.1.4 Огибающие
```{r}
library(boot)
envelopes <- function(s, mean, sd, r){
  x.qq <- qqnorm(s, plot.it = FALSE)
  x.qq <- lapply(x.qq, sort)
  x.gen <- function(dat, mle) rnorm(length(dat), mle[1], mle[2])
  x.qqboot <- boot(s, sort, R = r, sim = "parametric", ran.gen = x.gen, mle=c(mean, sd))
  x.env <- envelope(x.qqboot)
  plot(x.qq, main='Огибающие линии', xlab='', ylab='')
  lines(x.qq$x, x.env$point[1, ], lty = 4)
  lines(x.qq$x, x.env$point[2, ], lty = 4)
  lines(x.qq$x, x.env$overall[1, ], lty = 1)
  lines(x.qq$x, x.env$overall[2, ], lty = 1)
} 
```
```{r}
envelopes(s.1, 0, 25, 2000)
envelopes(s.2, 50, 10, 2000)
envelopes(s.3, 30, 10, 2000)
```

#### 2.1.5 Тесты нормальности
##### 2.1.5.1 Колмогорова-Смирнова
```{r}
ks.test(s.1, pnorm, mean=0, sd=25)
ks.test(s.2, pnorm, mean=50, sd=10)
ks.test(s.3, pnorm, mean=30, sd=10)
```

Значения статистики Колмогорова-Смирнова достаточно малы, значит выборки действительно из нормального распределения

##### 2.1.5.2 Шапиро-Уилка
```{r}
library(nortest)
shapiro.test(s.1)
shapiro.test(s.2)
shapiro.test(s.3)
```

Тесты Шапиро-Уилка дают результат близкий к 1, что тоже свидетельствует, что выборки из нормального распределения

##### 2.1.5.3 Андерсона-Дарлинга
```{r}
ad.test(s.1)
ad.test(s.2)
ad.test(s.3)
```


##### 2.1.5.4 Крамера-фон Мизеса
```{r}
cvm.test(s.1)
cvm.test(s.2)
cvm.test(s.3)
```

Статистика достаточно близка к 0, что говорит о нормальности распределения

##### 2.1.5.4 Лиллифорса (вариация Колмогорова-Смирнова именно для нормального распределения)
```{r}
lillie.test(s.1)
lillie.test(s.2)
lillie.test(s.3)
```

Статистика близка к 0, значит распределения имеют нормальный вид

##### 2.1.5.5 Шапиро-Франчия
```{r}
sf.test(s.1)
sf.test(s.2)
sf.test(s.3)
```

Результат близок к 1, что говорит о нормальности распределения

### 2.2 Выборки большого объема
#### 2.2.1 Генерирование и визуализация 
```{r}
set.seed(151021)
b.1 <- rnorm(2000, 150, 50)
b.2 <- rnorm(4000, 50, 10)
hist(b.2, col='lightblue', xlim=c(0,250), ylim=c(0,0.05), freq=FALSE, main='2 нормально распределенных выборки', xlab='', ylab='Плотность')
hist(b.1, col='lightgreen', add=TRUE, freq=FALSE)
lines(density(b.2), col='blue3', lwd=2)
lines(density(b.1), col='green3', lwd=2)
legend('topleft', 
       legend=c('1. ~ N(50,10), n=4000', '2. ~ N(150,50), n=1500'), 
       fill=c('blue', 'green'))
```

#### 2.2.2 Теоретические и эмпирические функции рапределения и плотности
```{r}
theor_empiric(b.1, 150, 50)
theor_empiric(b.2, 50, 10)
```

С увеличением объема эмпирические значение значительно приблизились к теоретическим

#### 2.2.3 Квантили
```{r}
quantile <- function(x){
  qqnorm(x)
  qqline(x, col='red')
}
```
```{r}
quantile(b.1)
quantile(b.2)
```

На Q-Q графиках так же заметно, что с увеличением выборки точки ближе прижимаются к прямой

#### 2.2.4 Огибающие
```{r}
envelopes(b.1, 150, 50, 5000)
envelopes(b.2, 50, 10, 5000)
```

#### 2.2.5 Тесты нормальности
##### 2.2.5.1 Колмогорова-Смирнова
```{r}
ks.test(b.1, pnorm, mean=150, sd=50)
ks.test(b.2, pnorm, mean=50, sd=10)
```

Значения статистики Колмогорова-Смирнова достаточно малы, значит выборки действительно из нормального распределения

##### 2.2.5.2 Шапиро-Уилка
```{r}
library(nortest)
shapiro.test(b.1)
shapiro.test(b.2)
```

Тесты Шапиро-Уилка дают результат близкий к 1, что тоже свидетельствует, что выборки из нормального распределения

#### 2.2.5.3 Андерсона-Дарлинга
```{r}
ad.test(b.1)
ad.test(b.2)
```


##### 2.2.5.4 Крамера-фон Мизеса
```{r}
cvm.test(b.1)
cvm.test(b.2)
```

Статистика достаточно близка к 0, что говорит о нормальности распределения

##### 2.2.5.5 Лиллифорса (вариация Колмогорова-Смирнова именно для нормального распределения)
```{r}
lillie.test(b.1)
lillie.test(b.2)
```

Статистика близка к 0, значит распределения имеют нормальный вид

##### 2.2.5.6 Шапиро-Франчия
```{r}
sf.test(b.1)
sf.test(b.2)
```

Результат близок к 1, что говорит о нормальности распределения

### 2.3 Анализ своих данных
Нормализованные данные по цене акций компании Chevron (CVX)
```{r}
data <- read.csv(file='..\\dataset.csv')
data$date <- as.Date(data$date)
```
```{r}
chevron <- subset(data, data['Name'] == 'CVX')
chevron$norm <- sapply(chevron$open,FUN=function(x){(x-mean(chevron$open))/(sd(chevron$open)*sqrt(nrow(chevron)))})
```

#### 2.3.1 Визуализация данных
```{r}
hist(chevron$norm, freq=FALSE, xlab='', ylab='', main='Нормализованная цена акции Chevron')
lines(density(chevron$norm), col='red')
```

#### 2.3.2 Теоретическая и эмпирическая функция рапределения и плотности
```{r}
theor_empiric(chevron$norm, mean(chevron$norm), sd(chevron$norm))
```

Визуально нормализация дает результат довольно близкий к теоретическому распределению

#### 2.3.3 Квантили
```{r}
quantile(chevron$norm)
```

На Q-Q графиках так же заметно, что значения, близкие к среднему выборки, нормализовались лучше, чем "хвосты" 

#### 2.3.4 Огибающие
```{r}
envelopes(chevron$norm, mean(chevron$norm), sd(chevron$norm), 3000)
```

Хвосты достаточно сильно выбиваются из "коридора" огибающих линий, так как данные не сгенерированы

#### 2.3.5 Тесты нормальности
##### 2.3.5.1 Колмогорова-Смирнова
```{r}
library(MASS)
x <- unique(chevron$norm)
fit <- fitdistr(x,"normal")
cvx.mean <- fit$estimate[1]
cvx.sd <- fit$estimate[2]
ks.test(x, pnorm, mean=cvx.mean, sd=cvx.sd)
```

Значение статистики Колмогорова-Смирнова достаточно мало, значит данные примерно удовлетворяют нормальному распределению 

##### 2.2.5.2 Шапиро-Уилка
```{r}
library(nortest)
shapiro.test(chevron$norm)
```

Тест Шапиро-Уилка даёт результат близкий к 1, что тоже свидетельствует, что данные удовлетворяют нормальному распределению

##### 2.3.5.3 Лиллифорса (вариация Колмогорова-Смирнова именно для нормального распределения)
```{r}
lillie.test(chevron$norm)
```

Статистика близка к 0, значит данные имеют нормальный вид

##### 2.3.5.4 Шапиро-Франчия
```{r}
sf.test(chevron$norm)
```

Значение статистики близко к 1, значит данные имеют нормальный вид

## 3. Выборки со случайными параметрами
#### 3.1. Генеирование и визуализация

2 выборки из гамма распределения с параметрами из нормального распределения
```{r}
set.seed(16102021)

par <- rnorm(10, 10, 10)

sm <- rgamma(100, abs(par[1]), abs(par[2]))
bg <- rgamma(5000, abs(par[3]), abs(par[4]))

par(mfrow=c(1,2))

hist(sm, freq=FALSE, col='salmon', main='n = 100', xlab='')
lines(density(sm), col='red3', lwd=2)

hist(bg, freq=FALSE, add=FALSE, col='lightblue', main='n = 5000', xlab='')
lines(density(bg), col='blue3', lwd=2)
```

#### 3.2. Теоретические и эмпирические функции рапределения и плотности
```{r}
fit.sm <- fitdistr(sm,"normal")
sm.mean <- fit.sm$estimate[1]
sm.sd <- fit.sm$estimate[2]

fit.bg <- fitdistr(bg,"normal")
bg.mean <- fit.bg$estimate[1]
bg.sd <- fit.bg$estimate[2]

theor_empiric(sm, sm.mean, sm.sd)
theor_empiric(bg, bg.mean, bg.sd)
```

Данные достаточно хорошо приближаются нормальным распределением

#### 3.3 Квантили
```{r}
quantile(sm)
quantile(bg)
```

На Q-Q графиках заметна вогнутость линии, значит, правый хвост распределения больше, чем левый (что справедливо для гамма-распределения)

#### 3.4 Огибающие
```{r}
envelopes(sm, sm.mean, sm.sd, 5000)
envelopes(bg, bg.mean, bg.sd, 5000)
```

Небольшая выборка хорошо аппроксимируется нормальным распределением, большая же - плохо, квантили выходят за "коридор" огибающих

#### 3.5 Тесты нормальности
##### 3.5.1 Колмогорова-Смирнова
```{r}
ks.test(sm, pnorm, mean=sm.mean, sd=sm.sd)
ks.test(bg, pnorm, mean=bg.mean, sd=bg.sd)
```

Значения статистики Колмогорова-Смирнова достаточно малы, значит выборки близки к нормальному распределению

##### 3.5.2 Шапиро-Уилка
```{r}
library(nortest)
shapiro.test(sm)
shapiro.test(bg)
```

Тесты Шапиро-Уилка дают результат близкий к 1, но меньший, чем они давали на заведомо нормальных данных (здесь уже можно усомниться в нормальности данных)

#### 3.5.3 Андерсона-Дарлинга
```{r}
ad.test(sm)
ad.test(bg)
```

На большой выборке тест дает слишком большой результат для нормального распределения

##### 3.5.4 Крамера-фон Мизеса
```{r}
cvm.test(sm)
cvm.test(bg)
```

Статистика на малой выборке достаточно близка к 0, что говорит о прибилженности к нормальному распределению

На большой же выборке результат сразу указывает на ненормальность данных

##### 3.5.5 Лиллифорса (вариация Колмогорова-Смирнова именно для нормального распределения)
```{r}
lillie.test(sm)
lillie.test(bg)
```

Статистика близка к 0, значит распределения имеют близкий к нормальному вид

##### 3.5.6 Шапиро-Франчия
```{r}
sf.test(sm)
sf.test(bg)
```

Результат близок к 1, что говорит о близости к нормальному распределению

### 3. Вывод

Выборки небольшого объема хорошо аппроксимируются нормальным распределением 

Сделать вывод о нормальности выборки большого объема лучше всего помогут:

1. Q-Q график
2. Метод огибающих
3. Метод Шапиро-Уилка
4. Тест Андерсона-Дарлинга
5. Тест Крамера-фон Мизеса
